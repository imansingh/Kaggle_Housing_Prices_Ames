---
title: "Ames_EDA_Linear_Regression"
author: "Iman Singh"
date: "2/1/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r import libraries}
library(ggplot2)
library(purrr)
library(dplyr)
library(mice)
library(VIM)
library(Hmisc)
library(MASS)
library(corrplot)
library(car)
library(glmnet)
library(gridExtra)
library(tabplot)
```

## Step 1: Understanding the Problem:

We are asked to predict house prices in Ames, Iowa based on 79 explanatory variables, and are given roughly equal sized training and test datasets (1460 and 1459 observations, respectively).


Even before looking at the data, my intuition is that multivariate linear regression techniques may be a good way to approach this problem, because the price of a house seems to depend on several factors added together, but that we wil have to pay special attention to feature selection in order to reduce the variance of our model, feature engineering. Also, response variable is numeric. 


```{r import data}
df_train <- read.csv("train.csv", 
                        header = TRUE, 
                        na.strings = c("", "NA"),
                        stringsAsFactors = TRUE)

df_test <- read.csv("test.csv", 
                        header = TRUE, 
                        na.strings = c("", "NA"),
                        stringsAsFactors = TRUE)

```

```{r drop Id and SalePrice}

# Save ID column
train_Id = df_train$Id
test_Id = df_test$Id

# Drop ID column since we can't use it as a feature
df_train = subset(df_train, select = -Id)
df_test = subset(df_test, select = -Id)

# Save SalePrice
response_variable = df_train$SalePrice

# Drop SalePrice so we don't accidentally use it as feature (when doing MICE imputation, for example)
df_train = subset(df_train, select = -SalePrice)

# Drop Utilities - it cant be used as a predictor because all values in training dataset are 'AllPub' except one 'NoSeWa'. All values in test dataset are 'AllPub' except two 'NA'. With such little variation (possibly none in test, depending on imputation), there is no predictive value
df_train = subset(df_train, select = -Utilities)
df_test = subset(df_test, select = -Utilities)
```



Missing Values
We used the below strategies for dealing with missing values

Flagging as 'None'
Features	Alley, BsmtCond, BsmtQual, BsmtExposure, BsmtFinType1, BsmtFinType2, Fence, Functional, FireplaceQu, GarageCond, GarageFinish, GarageQual, GarageType, , MiscFeature, PoolQC
 

# - ths one is problematic: MasVnrType, MSSubClass

In many cases, we want the model to treat observations with missing values as a separate category. For example, we know from the data description that a missing value for ‘PoolQC’ means that the house does not have a pool. It is important to let the algorithm know that some homes do not have pools, because this may affect their value, so we flag the missing values as ‘none’.

This same rationale applies to all but one these features - the house in question does not have the attribute being measured, so we enter the value as 'none'. The only exception is 'Functional' - we still want to flag the missing values for this feature, but we assign the value ‘typ’ instead of 'none' because the data description says that missing values here mean ‘typical functionality’.

Impute Zero 
Features	GarageYrBlt
 
This sorta works. Flags as different. Not sure of the statistical validity here. 

Impute the Mode
Features	 Exterior1st, Exterior2nd, KitchenQual, Electrical, MSZoning
 

For these categorial features, we knew the house had the attribute being measured, so we could not impute 'none'. In all the cases, there was one dominant value for most of the data and so we decided to impute the mode as the value of the missing data because, assuming the data are missing completely at random, it is probable that they (like most of the observations) have the most typical value.

Impute the Median by Neighborhood
Features	LotFrontage
 

For LotFrontage, we needed to impute a value because it does not make sense that a house is actually missing the attribute. Instead of imputing the median value for the entire dataset, we decided to impute the median for the neighborhood the house is located in to give us a more accurate estimate



```{r flag as none}
# Missing Values
# Flag as None
#	Alley, BsmtCond, BsmtQual, BsmtExposure, BsmtFinType1, BsmtFinType2, Fence, FireplaceQu, GarageCond, GarageFinish, GarageQual, GarageType, MiscFeature, PoolQC

# check missing values
sapply(df_train, function(x) sum(is.na(x)))
sapply(df_test, function(x) sum(is.na(x)))

flag_as_none = function(df) {
  df = df %>%
    mutate(Alley = ifelse(is.na(Alley), 'None', levels(Alley)),
           BsmtCond = ifelse(is.na(BsmtCond), 'None', levels(BsmtCond)),
           BsmtQual = ifelse(is.na(BsmtQual), 'None', levels(BsmtQual)),
           BsmtExposure = 
             ifelse(is.na(BsmtExposure), 'None', levels(BsmtExposure)),
           BsmtFinType1 = 
             ifelse(is.na(BsmtFinType1), 'None', levels(BsmtFinType1)),
           BsmtFinType2 = 
             ifelse(is.na(BsmtFinType2), 'None', levels(BsmtFinType2)),
           Fence = ifelse(is.na(Fence), 'None', levels(Fence)),
           FireplaceQu = 
             ifelse(is.na(FireplaceQu), 'None', levels(FireplaceQu)),
           GarageCond = 
             ifelse(is.na(GarageCond), 'None', levels(GarageCond)),
           GarageFinish = 
             ifelse(is.na(GarageFinish), 'None', levels(GarageFinish)),
           GarageQual = 
             ifelse(is.na(GarageQual), 'None', levels(GarageQual)),
           GarageType = 
             ifelse(is.na(GarageType), 'None', levels(GarageType)),
           MiscFeature = 
             ifelse(is.na(MiscFeature), 'None', levels(MiscFeature)),
           PoolQC = ifelse(is.na(PoolQC), 'None', levels(PoolQC))
           )
  df$Alley = as.factor(df$Alley)
  df$BsmtCond = as.factor(df$BsmtCond)
  df$BsmtQual = as.factor(df$BsmtQual)
  df$BsmtExposure = as.factor(df$BsmtExposure)
  df$BsmtFinType1 = as.factor(df$BsmtFinType1)
  df$BsmtFinType2 = as.factor(df$BsmtFinType2)
  df$Fence = as.factor(df$Fence)
  df$FireplaceQu = as.factor(df$FireplaceQu)
  df$FireplaceQu = as.factor(df$FireplaceQu)
  df$GarageCond = as.factor(df$GarageCond)
  df$GarageFinish = as.factor(df$GarageFinish)
  df$GarageQual = as.factor(df$GarageQual)
  df$GarageType = as.factor(df$GarageType)
  df$MiscFeature = as.factor(df$MiscFeature)
  df$PoolQC = as.factor(df$PoolQC)
  
  return(df)
}
df_train = flag_as_none(df_train)
df_test = flag_as_none(df_test)
```


```{r impute zero}
impute_zero = function(df){
  df = df %>%
    mutate(GarageYrBlt = ifelse(is.na(GarageYrBlt), 0, GarageYrBlt),
           BsmtFinSF1 = ifelse(is.na(BsmtFinSF1), 0, BsmtFinSF1),
           BsmtFinSF2 = ifelse(is.na(BsmtFinSF2), 0, BsmtFinSF2),
           BsmtUnfSF = ifelse(is.na(BsmtUnfSF), 0, BsmtUnfSF),
           BsmtFullBath = ifelse(is.na(BsmtFullBath), 0, BsmtFullBath),
           BsmtHalfBath = ifelse(is.na(BsmtHalfBath), 0, BsmtHalfBath),
           TotalBsmtSF = ifelse(is.na(TotalBsmtSF), 0, TotalBsmtSF),
           GarageCars = ifelse(is.na(GarageCars), 0, GarageCars), # this one is based on examination of the observation. Theory is that 'GarageType = 2Types' was entered in error
          GarageArea = ifelse(is.na(GarageArea), 0, GarageArea)
          )
}

df_train = impute_zero(df_train)
df_test = impute_zero(df_test)
```


Impute the other ones:
Electrical: 1
MasVnrType: 8
MasVnrArea: 8
LotFrontage: 259
MSZoning
Utilities
Exterior1st
Exterior2nd
MasVnrType
MasVnrArea
KitchenQual
Functional
SaleType



```{r impute values using MICE}

# impute using MICE

train_imputed = mice(df_train, m = 1, method = 'cart') # needed to change method from default
test_imputed = mice(df_test, m = 1, method = 'cart') # needed to change method from default

# check if data that was imputed is plausible
# LotFrontage
hist(train_imputed$imp$LotFrontage[[1]])
hist(df_train$LotFrontage)

hist(test_imputed$imp$LotFrontage[[1]])
hist(df_test$LotFrontage)

# MasVnrArea
hist(train_imputed$imp$MasVnrArea[[1]])
hist(df_train$MasVnrArea)

hist(test_imputed$imp$MasVnrArea[[1]])
hist(df_test$MasVnrArea)

# Electrical
barchart(train_imputed$imp$Electrical[[1]])
barchart(df_train$Electrical)

barchart(test_imputed$imp$Electrical[[1]])
barchart(df_test$Electrical)

# MasVnrType
barchart(train_imputed$imp$MasVnrType[[1]])
barchart(df_train$MasVnrType)

barchart(test_imputed$imp$MasVnrType[[1]])
barchart(df_test$MasVnrType)

# MSZoning
barchart(train_imputed$imp$MSZoning[[1]])
barchart(df_train$MSZoning)

barchart(test_imputed$imp$MSZoning[[1]])
barchart(df_test$MSZoning)

# Exterior1st
barchart(train_imputed$imp$Exterior1st[[1]])
barchart(df_train$Exterior1st)

barchart(test_imputed$imp$Exterior1st[[1]])
barchart(df_test$Exterior1st)

# Exterior2nd
barchart(train_imputed$imp$Exterior2nd[[1]])
barchart(df_train$Exterior2nd)

barchart(test_imputed$imp$Exterior2nd[[1]])
barchart(df_test$Exterior2nd)

# KitchenQual
barchart(train_imputed$imp$KitchenQual[[1]])
barchart(df_train$KitchenQual)

barchart(test_imputed$imp$KitchenQual[[1]])
barchart(df_test$KitchenQual)

# Functional
barchart(train_imputed$imp$Functional[[1]])
barchart(df_train$Functional)

barchart(test_imputed$imp$Functional[[1]])
barchart(df_test$Functional)

# SaleType
barchart(train_imputed$imp$SaleType[[1]])
barchart(df_train$SaleType)

barchart(test_imputed$imp$SaleType[[1]])
barchart(df_test$SaleType)

# Density Plots of imputed vs. non-imputed data (for numerical values)
densityplot(train_imputed)
densityplot(test_imputed)

# update data tables
df_train = complete(train_imputed, 1)
df_test = complete(test_imputed, 1)

```
```{r}
sapply(df_test, function(x) sum(is.na(x)))
```

Let's check if they were imported as the right classes

```{r check class}
map_chr(df_train, class)
map_chr(df_test, class)
```
Looks like we need to change some of these classes!

I want to make sure anything that tells us about the *type* of something is classified as a 'factor', and anything that tells us the *amount* of something is classified as an 'integer'. 

It looks like everything that has already been classified as a factor is in the proper category, but several of the features have been incorrectly classified as integers.

But, before doing that we are going to first derive some features because we are going to take advantage of some features (the ones measuring the number of bathrooms, bedrooms and kitchens), being initially classified as integers. I  provide an argument for why these features should really be classified as factors, below.

Derived Features:

We can derive:
OthrRmsAbvGr = TotalRms - (KitchenAbvGr + BedroomAbvGr + Bathroom + HalfBath)
Adding all baths
Adding Square Footage
Adjusting for High Quality / Low Quality

Others:
OutdoorSF = WoodDeckSF + OpenPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch

```{r derived features}
# wrap it in a function so we can easily change df_test in the same way

get_derived_features = function(df){
 df = df %>%
   mutate(TotalBaths = 
            as.numeric(BsmtFullBath) + (as.numeric(BsmtHalfBath) * .5) + 
            as.numeric(FullBath) + (as.numeric(HalfBath) * .5),
          TotalSF = 
            TotalBsmtSF + GrLivArea,
          HighQualFinSF = 
            TotalSF - LowQualFinSF,
          OthrRmsAbvGrd = 
            TotRmsAbvGrd -
            (as.numeric(KitchenAbvGr) + as.numeric(BedroomAbvGr)),
          OutdoorSF =
            (WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch),
          # Min date for YearRemodAdd = 1950. We do not have info about homes remodeled
          # before 1950, so this captures info in case older homes were never remodeled
          YearLastUpdated =  
            ifelse(YearRemodAdd > 1950, YearRemodAdd, YearBuilt)
          )
}
df_train = get_derived_features(df_train)
df_test = get_derived_features(df_test)
```





The features classified as integers should measure an amount, not a type. Therefore, all the features that measure square or linear footage are properly labeled as an integers (`LotFrontage`, `LotArea`, etc). There are also two features that measure price, which is an amount, and so are correctly labeled: `SalePrice` (our response variable), and `MiscVal` (even though this feature is categorized correctly, it is problematic in other ways that I will discuss below). 

There are a few unambiguous cases of features mis-labeled as integers:
`MSSubclass` - identifies the type of dwelling, and so is clearly categorical
`MoSold` - identifies the month sold (1-12), and so is clearly categorical
`OverallQual` - rates the overall material and finish of the house (1-10). The rating assigns a type to the house quality, not an amount
`OverallCond` - rates the overall condition of the house(1-10). Again, this assigns a type to the house condition, not an amount. 

There are additional cases that (to me) are mis-labeled as integers:
`BsmtFullBath`, `BsmtHalfBath`, `FullBath`, `HalfBath`, `BedroomAbvGr`, `KitchenAbvGr`, `TotRmsAbvGrd`, `Fireplaces`, `GarageCars`
All the features listed above actually do measure the amount of things - bathrooms, bedrooms, kitchens, fireplaces, cars. So they have a case for being counted as integers. However, I would argue that, in the context of buying a house, they actually identify different types. I think, in the context of home value, a 2-bathroom house is a different type of house than a 3-bathroom house. Or, similarly, a 2-car garage is a different type than a 3-car garage. These features don't really measure a difference in degree, but type.
To highlight the difference, I would argue that the squre footage features, correctly labeled as integers, do measure degree. A 2000 square foot house is different in degree from a 2500 square foot house, and each step along the continuum changes the degree. On the other hand, each additional bathroom changes the type of house from a 1-bathroom type, to a 2-bathroom type, etc.
My argument works for most of the variables listed. I'm not confident it works for `TotalRoomsAbvGrd` or `Fireplaces` though. Once you're counting all the rooms, is a 7-room house different in type than a 8-room house? I'm not sure. And is a 1-fireplace house really different in type from a 2-fireplace house? Again, I'm not sure. As a compromise, I'll treat the total rooms value as an integer, but each of the specific rooms (Kitchen, Bath, Bedroom, Other) as factors. Similarly, I'll treat totalBaths as an iteger, but the specific full-bath and half-bath features as categorical. 

(we will eventually change bathrooms to factors, but first we will do some feature engineering)

There is a type of feature that isn't really an integer or a factor: the 'Year' features:
     `YearBuilt`  `YearRemodAdd`  `GarageYrBlt` `YrSold`
  



       
 

`MiscVal` is an especially interesting feature because it is the dolar value of a miscellaneous feature (specified under `MiscFeature`) not covered by the other features. This feature is in the same units, dollars, as the response variable and is specifically supposed to capture value not captured by those features. 
Because of this, it may be possible to train our model using all the features except `MiscFeature` and `MiscVal` and the response variable `SalePrice - MiscVal`, and then add the values from `MiscVal` back into the results before giving the predicted `SalePrice`. 




```{r convert class}

convert_classes = function(df){
  df$MSSubClass = as.factor(df$MSSubClass)
  df$MoSold = as.factor(df$MoSold)
  df$OverallQual = as.factor(df$OverallQual)
  df$OverallCond = as.factor(df$OverallCond)

  # only do these after deriving features
  df$BsmtFullBath = as.factor(df$BsmtFullBath)
  df$BsmtHalfBath = as.factor(df$BsmtHalfBath)
  df$FullBath = as.factor(df$FullBath)
  df$HalfBath = as.factor(df$HalfBath)
  df$BedroomAbvGr = as.factor(df$BedroomAbvGr)
  df$KitchenAbvGr = as.factor(df$KitchenAbvGr)
  df$Fireplaces = as.factor(df$Fireplaces)
  df$GarageCars = as.factor(df$GarageCars)
  return(df)
}

df_train = convert_classes(df_train)
df_test = convert_classes(df_test)
```



In addition to the ones below, should do 
basementfinsf X basmentFinType (need to bin)
# PoolArea * PoolQC (need to bin)
MasVnrType * MasVnrArea (need to bin)

Kitchen * KitchenQual
Fireplaces * FireplaceQu
GarageType * GarageQual
GarageQual * GarageCond
GarageType * GarageCond
LandContour * LandSlope
Condition1 * Condition2 

```{r feature interactions}
# feature interactions
# OverallQuality*OverallCond	
# ExterQual*ExterCond
# BsmtQual*BsmtCond	GarageQual*GarageCond
# Heating*HeatingQC	SaleType*SaleCondition
# Neighborhood*BldgType
hist(df_train$MasVnrArea[df_train$MasVnrArea > 0])
hist(df_test$MasVnrArea[df_test$MasVnrArea > 0])

#Divide BsmtFinSF1
BsmtFinSF1_cut = cut2(df_test$BsmtFinSF1) 
levels(BsmtFinSF1_cut) = c(seq(1:8))

#Divide BsmtFinSF2
BsmtFinSF2_cut = cut2(df_test$BsmtFinSF2) 
levels(BsmtFinSF2_cut) = c(seq(1:3))

# Divide MasVnrArea into bins
MasVnrArea_cut = cut2(df_test$MasVnrArea) 
levels(MasVnrArea_cut) = c(seq(1:5)) # change levels to something easier to print

get_interaction_features = function(df){
 df = df %>%
   mutate(OverallQualXOverallCond = 
            paste(OverallQual, '*', OverallCond, sep = ''),
          ExterQualXExterCond = 
            paste(ExterQual, '*', ExterCond, sep = ''),
          BsmtQualXBsmtCond = 
            paste(BsmtQual, '*', BsmtCond, sep = ''),
          BsmtFinSF1XBsmtFinType1 = 
            paste(BsmtFinSF1_cut, '*', BsmtFinType1, sep = ''),
          BsmtFinSF2XBsmtFinType2 = 
            paste(BsmtFinSF2_cut, '*', BsmtFinType2, sep = ''),
          GarageQualXGarageCond = 
            paste(GarageQual, '*', GarageCond, sep = ''),
          HeatingXHeatingQC = 
            paste(Heating, '*', HeatingQC, sep = ''),
          SaleTypeXSaleCondition = 
            paste(SaleType, '*', SaleCondition, sep = ''),
          NeighborhoodXBldgType = 
            paste(Neighborhood, '*', BldgType, sep = ''),
          MasVnrAreaXMasVnrType = 
            paste(MasVnrArea_cut, '*', MasVnrType, sep = ''),
          KitchenAbvGrXKitchenQual = 
            paste(KitchenAbvGr, '*', KitchenQual, sep = ''),
          FireplacesXFireplaceQu = 
            paste(Fireplaces, '*', FireplaceQu, sep = ''),
          GarageTypeXGarageQual = 
            paste(GarageType, '*', GarageQual, sep = ''),
          GarageTypeXGarageCond = 
            paste(GarageType, '*', GarageCond, sep = ''),
          GarageQualXGarageCond = 
            paste(GarageQual, '*', GarageCond, sep = ''),
          LandContourXLandSlope = 
            paste(LandContour, '*', LandSlope, sep = ''),
          Condition1XCondition2 = 
            paste(Condition1, '*', Condition2, sep = ''),
          RoofStyleXRoofMatl = 
            paste(RoofStyle, '*', RoofMatl, sep = '')
          )
 
 # convert classes to factor
 df$OverallQualXOverallCond = as.factor(df$OverallQualXOverallCond)
 df$ExterQualXExterCond = as.factor(df$ExterQualXExterCond)
 df$BsmtQualXBsmtCond = as.factor(df$BsmtQualXBsmtCond)
 df$BsmtFinSF1XBsmtFinType1 = as.factor(df$BsmtFinSF1XBsmtFinType1)
 df$BsmtFinSF2XBsmtFinType2 = as.factor(df$BsmtFinSF2XBsmtFinType2)
 df$GarageQualXGarageCond = as.factor(df$GarageQualXGarageCond)
 df$HeatingXHeatingQC = as.factor(df$HeatingXHeatingQC)
 df$NeighborhoodXBldgType = as.factor(df$NeighborhoodXBldgType)
 df$MasVnrAreaXMasVnrType = as.factor(df$MasVnrAreaXMasVnrType)
 df$KitchenAbvGrXKitchenQual = as.factor(df$KitchenAbvGrXKitchenQual)
 df$FireplacesXFireplaceQu = as.factor(df$FireplacesXFireplaceQu)
 df$GarageTypeXGarageQual = as.factor(df$GarageTypeXGarageQual)
 df$GarageTypeXGarageCond = as.factor(df$GarageTypeXGarageCond)
 df$GarageQualXGarageCond = as.factor(df$GarageQualXGarageCond)
 df$LandContourXLandSlope = as.factor(df$OverallQualXOverallCond)
 df$Condition1XCondition2 = as.factor(df$OverallQualXOverallCond)
 df$RoofStyleXRoofMatl = as.factor(df$RoofStyleXRoofMatl)
 
 return(df)
}

df_train = get_interaction_features(df_train)
df_test = get_interaction_features(df_test)
```


## Correlation Matrix

Log transform:
respose_variable
LotFrontage
LotArea
MasVnrArea
BsmtFinSF1
BsmtUnfSF
TotalBsmtSF
X1stFlrSF
X2ndFlrSF
LowQualFinSF
GrLivArea
TotRmsAbvGrd
GarageArea
WoodDeckSF
OpenPorchSF
EnclosedPorch
X3SsnPorch
ScreenPorch
PoolArea
MiscVal
TotalBaths
TotalSF
HighQualFinSF
OthrRmsAbvGrd + 2
OutdoorSF


## Data Transformations


```{r log transformation}
# check distributions of target variable and all numerics 
log_transformation = function(df) {
  df = df %>%
    mutate(LotArea = log(LotArea),
           LotFrontage = log(LotFrontage),
           MasVnrArea = log(MasVnrArea),
           BsmtFinSF1 = log(BsmtFinSF1),
           BsmtFinSF2 = log(BsmtFinSF2),
           TotalBsmtSF = log(TotalBsmtSF),
           X1stFlrSF = log(X1stFlrSF),
           X2ndFlrSF = log(X2ndFlrSF),
           LowQualFinSF = log(LowQualFinSF),
           GrLivArea = log(GrLivArea),
           TotRmsAbvGrd = log(TotRmsAbvGrd),
           GarageArea = log(GarageArea),
           WoodDeckSF = log(WoodDeckSF),
           OpenPorchSF = log(OpenPorchSF),
           EnclosedPorch = log(EnclosedPorch),
           X3SsnPorch = log(X3SsnPorch),
           ScreenPorch = log(ScreenPorch),
           PoolArea = log(PoolArea),
           MiscVal = log(MiscVal),
           TotalBaths = log(TotalBaths),
           TotalSF = log(TotalSF),
           HighQualFinSF = log(HighQualFinSF),
           OthrRmsAbvGrd = log(OthrRmsAbvGrd),
           OutdoorSF = log(OutdoorSF)
    )
}
           
# change response_variable to log(response_variable) - need to remember to convert back with exp(response_variable) when predicting prices
response_variable = log(response_variable)

# modify test and train dataframes
df_train = log_transformation(df_train)
df_test = log_transformation(df_test)
```

```{r}
names(df_train)
```

```{r}
map_chr(df_train, class)
map_chr(df_test, class)

```

Categorical:
MSSubclass
MSZoning
Street
Alley
LotShape
LandContour
LotConfig
LandSlope
Neighborhood
Condition1
Condition2
BldgType
HouseStyle
OverallQual
OverallCond
RoofStyle
RoofMatl
Exterior1st
Exterior2nd
MasVnrType
ExterQual
ExterCond
Foundation
BsmtQual
BsmtCond
BsmtExposure
BsmtFinType1
BsmtFinType2
Heating
HeatingQC
CentralAir
Electrical
BsmtFullBath
BsmtHalfBath
FullBath
HalfBath
BedroomAbvGr
KitchenAbvGr
KitchenQual
Functional
Fireplaces
FireplaceQu
GarageType
GarageFinish
GarageCars
GarageQual
GarageCond
PavedDrive
PoolQC
Fence
MiscFeature
MoSold
SaleType
SaleCondition
OverallQualXOverallCond
ExterQualXExterCond
BsmtQualXBsmtCond
BsmtFinSF1XBsmtFinType1
BsmtFinSF2XBsmtFinType2
GarageQualXGarageCond
HeatingXHeatingQC
SaleTypeXSaleCondition
NeighborhoodXBldgType
MasVnrAreaXMasVnrType
KitchenAbvGrXKitchenQual
FireplacesXFireplaceQu
GarageTypeXGarageQual
GarageTypeXGarageCond
LandContourXLandSlope
Condition1XCondition2
RoofStyleXRoofMatl



Numeric:
LotFrontage
LotArea
YearBuilt
YearRemodAdd
MasVnrArea
BsmtFinSF1
BsmtFinSF2
BsmtUnfSF
TotalBsmtSF
X1stFlrSF
X2ndFlrSF
LowQualFinSF
GrLivArea
TotRmsAbvGrd
GarageYrBlt
GarageArea
WoodDeckSF
OpenPorchSF
EnclosedPorch
X3SsnPorch
ScreenPorch
PoolArea
MiscVal
YrSold
TotalBaths
TotalSF
HighQualFinSF
OthrRmsAbvGrd
OutdoorSF



```{r}

## when training data on interaction featuress, make sure that some interaction feature exists in the test dataset. Sometimes there is't a map between 
tr = ggplot(data = df_train)
te = ggplot(data = df_test)

grid.arrange(tr + geom_bar(aes(x=MSSubClass)), 
             te + geom_bar(aes(x=MSSubClass)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=MSZoning)), 
             te + geom_bar(aes(x=MSZoning)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Street)), 
             te + geom_bar(aes(x=Street)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Alley)), 
             te + geom_bar(aes(x=Alley)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=LotShape)), 
             te + geom_bar(aes(x=LotShape)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=LandContour)), 
             te + geom_bar(aes(x=LandContour)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=LotConfig)), 
             te + geom_bar(aes(x=LotConfig)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=LandSlope)), 
             te + geom_bar(aes(x=LandSlope)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Neighborhood)), 
             te + geom_bar(aes(x=Neighborhood)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Condition1)), 
             te + geom_bar(aes(x=Condition1)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Condition2)), 
             te + geom_bar(aes(x=Condition2)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BldgType)), 
             te + geom_bar(aes(x=BldgType)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=HouseStyle)), 
             te + geom_bar(aes(x=HouseStyle)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=OverallQual)), 
             te + geom_bar(aes(x=OverallQual)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=OverallCond)), 
             te + geom_bar(aes(x=OverallCond)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=RoofStyle)), 
             te + geom_bar(aes(x=RoofStyle)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=RoofMatl)), 
             te + geom_bar(aes(x=RoofMatl)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Exterior1st)), 
             te + geom_bar(aes(x=Exterior1st)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Exterior2nd)), 
             te + geom_bar(aes(x=Exterior2nd)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=ExterCond)), 
             te + geom_bar(aes(x=ExterCond)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=ExterQual)), 
             te + geom_bar(aes(x=ExterQual)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Foundation)), 
             te + geom_bar(aes(x=Foundation)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtQual)), 
             te + geom_bar(aes(x=BsmtQual)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtCond)), 
             te + geom_bar(aes(x=BsmtCond)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtExposure)), 
             te + geom_bar(aes(x=BsmtExposure)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtFinType1)), 
             te + geom_bar(aes(x=BsmtFinType1)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtFinType2)), 
             te + geom_bar(aes(x=BsmtFinType2)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Heating)), 
             te + geom_bar(aes(x=Heating)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=HeatingQC)), 
             te + geom_bar(aes(x=HeatingQC)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=CentralAir)), 
             te + geom_bar(aes(x=CentralAir)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Electrical)), 
             te + geom_bar(aes(x=Electrical)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtFullBath)), 
             te + geom_bar(aes(x=BsmtFullBath)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtHalfBath)), 
             te + geom_bar(aes(x=BsmtHalfBath)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=FullBath)), 
             te + geom_bar(aes(x=FullBath)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=HalfBath)), 
             te + geom_bar(aes(x=HalfBath)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BedroomAbvGr)), 
             te + geom_bar(aes(x=BedroomAbvGr)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=KitchenAbvGr)), 
             te + geom_bar(aes(x=KitchenAbvGr)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=KitchenQual)), 
             te + geom_bar(aes(x=KitchenQual)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Functional)), 
             te + geom_bar(aes(x=Functional)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Fireplaces)), 
             te + geom_bar(aes(x=Fireplaces)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=FireplaceQu)), 
             te + geom_bar(aes(x=FireplaceQu)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=GarageType)), 
             te + geom_bar(aes(x=GarageType)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=GarageFinish)), 
             te + geom_bar(aes(x=GarageFinish)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=GarageCars)), 
             te + geom_bar(aes(x=GarageCars)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=GarageQual)), 
             te + geom_bar(aes(x=GarageQual)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=GarageCond)), 
             te + geom_bar(aes(x=GarageCond)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=PavedDrive)), 
             te + geom_bar(aes(x=PavedDrive)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=PoolQC)), 
             te + geom_bar(aes(x=PoolQC)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Fence)), 
             te + geom_bar(aes(x=Fence)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=MiscFeature)), 
             te + geom_bar(aes(x=MiscFeature)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=MoSold)), 
             te + geom_bar(aes(x=MoSold)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=SaleType)), 
             te + geom_bar(aes(x=SaleType)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=SaleCondition)), 
             te + geom_bar(aes(x=SaleCondition)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=OverallQualXOverallCond)), 
             te + geom_bar(aes(x=OverallQualXOverallCond)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=ExterQualXExterCond)), 
             te + geom_bar(aes(x=ExterQualXExterCond)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtQualXBsmtCond)), 
             te + geom_bar(aes(x=BsmtQualXBsmtCond)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtFinSF1XBsmtFinType1)), 
             te + geom_bar(aes(x=BsmtFinSF1XBsmtFinType1)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=BsmtFinSF2XBsmtFinType2)), 
             te + geom_bar(aes(x=BsmtFinSF2XBsmtFinType2)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=HeatingXHeatingQC)), 
             te + geom_bar(aes(x=HeatingXHeatingQC)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=SaleTypeXSaleCondition)), 
             te + geom_bar(aes(x=SaleTypeXSaleCondition)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=NeighborhoodXBldgType)), 
             te + geom_bar(aes(x=NeighborhoodXBldgType)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=MasVnrAreaXMasVnrType)), 
             te + geom_bar(aes(x=MasVnrAreaXMasVnrType)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=KitchenAbvGrXKitchenQual)), 
             te + geom_bar(aes(x=KitchenAbvGrXKitchenQual)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=FireplacesXFireplaceQu)), 
             te + geom_bar(aes(x=FireplacesXFireplaceQu)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=GarageTypeXGarageQual)), 
             te + geom_bar(aes(x=GarageTypeXGarageQual)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=GarageTypeXGarageCond)), 
             te + geom_bar(aes(x=GarageTypeXGarageCond)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=LandContourXLandSlope)), 
             te + geom_bar(aes(x=LandContourXLandSlope)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=Condition1XCondition2)), 
             te + geom_bar(aes(x=Condition1XCondition2)), 
             ncol = 2)

grid.arrange(tr + geom_bar(aes(x=RoofStyleXRoofMatl)), 
             te + geom_bar(aes(x=RoofStyleXRoofMatl)), 
             ncol = 2)

```

```{r}

grid.arrange(tr + geom_histogram(aes(x=LotFrontage)), 
             te + geom_histogram(aes(x=LotFrontage)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=LotArea)), 
             te + geom_histogram(aes(x=LotArea)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=YearBuilt)), 
             te + geom_histogram(aes(x=YearBuilt)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=YearRemodAdd)), 
             te + geom_histogram(aes(x=YearRemodAdd)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=YearLastUpdated)), 
             te + geom_histogram(aes(x=YearLastUpdated)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=MasVnrArea)), 
             te + geom_histogram(aes(x=MasVnrArea)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=BsmtFinSF1)), 
             te + geom_histogram(aes(x=BsmtFinSF1)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=BsmtFinSF2)), 
             te + geom_histogram(aes(x=BsmtFinSF2)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=BsmtUnfSF)), 
             te + geom_histogram(aes(x=BsmtUnfSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=TotalBsmtSF)), 
             te + geom_histogram(aes(x=TotalBsmtSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=X1stFlrSF)), 
             te + geom_histogram(aes(x=X1stFlrSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=X2ndFlrSF)), 
             te + geom_histogram(aes(x=X2ndFlrSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=LowQualFinSF)), 
             te + geom_histogram(aes(x=LowQualFinSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=GrLivArea)), 
             te + geom_histogram(aes(x=GrLivArea)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=TotRmsAbvGrd)), 
             te + geom_histogram(aes(x=TotRmsAbvGrd)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=GarageYrBlt)), 
             te + geom_histogram(aes(x=GarageYrBlt)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=GarageArea)), 
             te + geom_histogram(aes(x=GarageArea)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=WoodDeckSF)), 
             te + geom_histogram(aes(x=WoodDeckSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=OpenPorchSF)), 
             te + geom_histogram(aes(x=OpenPorchSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=EnclosedPorch)), 
             te + geom_histogram(aes(x=EnclosedPorch)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=X3SsnPorch)), 
             te + geom_histogram(aes(x=X3SsnPorch)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=ScreenPorch)), 
             te + geom_histogram(aes(x=ScreenPorch)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=PoolArea)), 
             te + geom_histogram(aes(x=PoolArea)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=MiscVal)), 
             te + geom_histogram(aes(x=MiscVal)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=YrSold)), 
             te + geom_histogram(aes(x=YrSold)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=TotalBaths)), 
             te + geom_histogram(aes(x=TotalBaths)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=TotalSF)), 
             te + geom_histogram(aes(x=TotalSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=HighQualFinSF)), 
             te + geom_histogram(aes(x=HighQualFinSF)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=OthrRmsAbvGrd)), 
             te + geom_histogram(aes(x=OthrRmsAbvGrd)), 
             ncol = 2)

grid.arrange(tr + geom_histogram(aes(x=OutdoorSF)), 
             te + geom_histogram(aes(x=OutdoorSF)), 
             ncol = 2)

```

```{r}
colMtx <- matrix(names(df_train)[1:length(df_train)-1], nrow = 6)
for (i in 1:ncol(colMtx)) {
  tableplot(df_train, 
            select_string = c(colMtx[,i], "SalePrice"), 
            sortCol = "SalePrice", decreasing = TRUE, 
            nBins = 30)
}
```


```{r}
ggplot(df_train[df_train$PoolArea != 0, ], aes(x = PoolArea)) +
  #stat_count()
    geom_histogram()
summary(df_train$PoolArea)
sum(is.na(df_train$PoolArea))
```

```{r}

ggplot(response_variable, aes(x = response_variable)) +
  geom_histogram()

hist(response_variable)
hist(log(response_variable))


sapply(df_train, function(x) sum(is.nan(x)))
```





```{r}
model = lm(response_variable ~ ., data = df_train)
summary(model)
```

```{r}
plot(model)
vif(model) # doesn't work - perfect multicollinerarity
av.plots(model)

model.full = model
model.empty = lm(response_variable ~ 1, data = df_train)
scope = list(lower = formula(model.empty), upper = formula(model.full))


#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
backwardAIC = step(model.full, scope, direction = "backward", k = 2)
bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)

#Stepwise regression using BIC as the criteria (the penalty k = log(n)).
forwardBIC = step(model.empty, scope, direction = "forward", k = log(1460))
backwardBIC = step(model.full, scope, direction = "backward", k = log(1460))
bothBIC.empty = step(model.empty, scope, direction = "both", k = log(1460))
bothBIC.full = step(model.full, scope, direction = "both", k = log(1460))

```
```{r}
influencePlot(forwardAIC)
```

```{r}
#Need matrices for glmnet() function. Automatically conducts conversions as well
#for factor variables into dummy variables.
x = model.matrix(response_variable ~ ., df_train)[, -1] #Dropping the intercept column.
y = response_variable

#Values of lambda over which to check.
grid = 10^seq(5, -2, length = 100)

#Fitting the ridge regression. Alpha = 0 for ridge regression.
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)

dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
coef(ridge.models) #Inspecting the various coefficient estimates.

#What do the estimates look like for a smaller value of lambda?
ridge.models$lambda[80] #Lambda = 0.2595.
coef(ridge.models)[, 80] #Estimates not close to 0.
sqrt(sum(coef(ridge.models)[-1, 80]^2)) #L2 norm is 136.8179.

#What do the estimates look like for a larger value of lambda?
ridge.models$lambda[15] #Lambda = 10,235.31.
coef(ridge.models)[, 15] #Most estimates close to 0.
sqrt(sum(coef(ridge.models)[-1, 15]^2)) #L2 norm is 7.07.

#Visualizing the ridge regression shrinkage.
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")

#Can use the predict() function to obtain ridge regression coefficients for a
#new value of lambda, not necessarily one that was within our grid:
predict(ridge.models, s = 50, type = "coefficients")

#Creating training and testing sets. Here we decide to use a 70-30 split with
#approximately 70% of our data in the training set and 30% of our data in the
#test set.
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]

length(train)/nrow(x)
length(y.test)/nrow(x)

#Let's attempt to fit a ridge regression using some arbitrary value of lambda;
#we still have not yet figured out what the best value of lambda should be!
#We will arbitrarily choose 5. We will now use the training set exclusively.
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2)

#Here, the MSE is approximately 115,541.

#What would happen if we fit a ridge regression with an extremely large value
#of lambda? Essentially, fitting a model with only an intercept:
ridge.largelambda = predict(ridge.models.train, s = 1e10, newx = x[test, ])
mean((ridge.largelambda - y.test)^2)

#Here, the MSE is much worse at aproximately 208,920.

#Instead of arbitrarily choosing random lambda values and calculating the MSE
#manually, it's a better idea to perform cross-validation in order to choose
#the best lambda over a slew of values.

#Running 10-fold cross validation.
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train],
                         lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)

#What is the test MSE associated with this best value of lambda?
ridge.bestlambdatrain = predict(ridge.models.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)

#Here the MSE is lower at approximately 113,173; a further improvement
#on that which we have seen above. With "cv.ridge.out", we can actually access
#the best model from the cross validation without calling "ridge.models.train"
#or "bestlambda.ridge":
ridge.bestlambdatrain = predict.cv.glmnet(cv.ridge.out, s ="lambda.min", newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)

```

